{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2011e45a-f164-48e4-807d-05ba9807937d",
   "metadata": {},
   "source": [
    "#  Práctica Obligatoria - Parte III - CNN\n",
    "\n",
    "***<p style=\"text-align:center;\">Aprendizaje Automático II</p>***\n",
    "***<p style=\"text-align:center;\">CNNs</p>***\n",
    "\n",
    "En esta parte, entrenarás varias redes CNNs y analizarás sus resultados en el dataset GTS. \n",
    "\n",
    "\n",
    "### Evaluación - 4/10 puntos\n",
    "\n",
    "Puntuación de cada parte sobre el total de la práctica:\n",
    "- **[Ejercicio 1]** 0.25 puntos.\n",
    "- **[Ejercicio 2]** 0.25 puntos.\n",
    "- **[Ejercicio 3]** 0.5 puntos.\n",
    "- **[Ejercicio 4]** 3 puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91107848-bd85-4af8-b6f8-89635f7fd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from gts_dataset import GTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60f44b-92f4-4a96-afa2-86512f267e60",
   "metadata": {},
   "source": [
    "## Importa las funciones que has creado desde `utils.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228fbee-b513-400d-b01d-cb9bc75a84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import count_trainable_params, train, evaluate, train_and_evaluate, save_full_model, load_full_model, plot_loss_accuracy, plot_confusion_matrix, plot_error_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03840dc-4f77-4c36-ac90-6346498df85b",
   "metadata": {},
   "source": [
    "## Carga el dataset GTS  y crea los dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640046c-1a5e-435a-8f69-a5cc2e0a4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las transformaciones y conversión a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = GTS(csv_file='train.csv', root_dir='./data', transform=transform)\n",
    "valid_dataset = GTS(csv_file='valid.csv', root_dir='./data', transform=transform)\n",
    "test_dataset = GTS(csv_file='test.csv', root_dir='./data', transform=transform)\n",
    "\n",
    "device = 'cpu'\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301c40d-5f47-4863-b79e-fc8ccf03e5d7",
   "metadata": {},
   "source": [
    "## **[Ejercicio 1]** Crea un modelo `CNN` con las siguientes características:\n",
    "\n",
    "1. Tres bloques convolucionales (conv2d -> batch norm -> ReLU).\n",
    "2. Flatten para aplanar características.\n",
    "3. Una capa densa para clasificación.\n",
    "4. Comprueba el número de parámetros entrenables y busca una configuración que no supere los **50.000** parámetros (aproximadamente 4 veces menos que la MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207e6f3-684b-4090-89c0-50b67c751aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, ouput_dim):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Primer bloque convolucional\n",
    "       \n",
    "        # Segundo bloque convolucional\n",
    "       \n",
    "        # Tercer bloque convolucional\n",
    "\n",
    "        # Capa de aplanado\n",
    "\n",
    "        # Linear para la clasificación\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Primer bloque convolucional\n",
    "       \n",
    "\n",
    "        # Segundo bloque convolucional\n",
    "      \n",
    "\n",
    "        # Tercer bloque convolucional\n",
    "      \n",
    "\n",
    "        # Aplanar la salida de la última capa convolucional\n",
    "      \n",
    "        # Linear para clasificación\n",
    "\n",
    "        return x\n",
    "\n",
    "count_trainable_params(CNN(ouput_dim=43))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b88268-4583-41d7-b835-2106ca225d2d",
   "metadata": {},
   "source": [
    "Guarda el modelo en `models.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039db3b0-a3a9-472b-a902-d70997d04178",
   "metadata": {},
   "source": [
    "### Entrena el modelo CNN con las funciones que has importado y analiza los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe63cb-62f6-445b-bed6-7f89f635660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia el modelo\n",
    "model_cnn = \n",
    "\n",
    "# Instancia la función de pérdida y el optimizador\n",
    "loss_fn = \n",
    "optimizer = \n",
    "\n",
    "# Fija 10 épocas\n",
    "num_epochs = 10\n",
    "\n",
    "# Entrena y evalua el modelo. Usa name='cnn'\n",
    "train_loss_df, val_loss_df, test_loss, test_accuracy = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Guarda el modelo con file_name='cnn.pth'\n",
    "\n",
    "\n",
    "# Carga los datos y muéstralos\n",
    "train_loss_file = 'train_loss_cnn.csv'  \n",
    "valid_loss_file = 'valid_loss_cnn.csv'\n",
    "train_accuracy_file = 'train_accuracy_cnn.csv'\n",
    "valid_accuracy_file = 'valid_accuracy_cnn.csv'\n",
    "\n",
    "plot_loss_accuracy(train_loss_file, valid_loss_file, train_accuracy_file, valid_accuracy_file)\n",
    "\n",
    "# Calcula la matriz de confusión\n",
    "cm = plot_confusion_matrix(model_cnn, test_loader, device=device)\n",
    "\n",
    "# Muestra el porcentaje de error por clase\n",
    "plot_error_per_class(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d583350-8e12-4b84-931a-3557656dbb51",
   "metadata": {},
   "source": [
    "Describe las diferencias en los resultados que has obtenido.\n",
    "\n",
    "############## COMPLETAR ##############\n",
    "\n",
    "############ Fin COMPLETAR ############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d3b98-78a4-45c1-8b22-a1244ac59c57",
   "metadata": {},
   "source": [
    "## **[Ejercicio 2]** Dataset permutado: re-entrena la MLP y la CNN\n",
    "\n",
    "Esta vez, vuelve a entrenar la `MLP`y la `CNN` que has configurado pero con el mismo dataset modificado.\n",
    "\n",
    "Esta modificación consiste en permutar los píxeles de las imágenes siempre de la misma manera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75818bda-c405-42e3-888f-8b324c9e0dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_subset(dataset, num_images=6):\n",
    "    \"\"\"\n",
    "    Visualiza ejemplos directamente de un Subset o Dataset.\n",
    "    \n",
    "    :param dataset: Dataset o Subset que contiene las imágenes y etiquetas.\n",
    "    :param num_images: Número de imágenes a mostrar.\n",
    "    \"\"\"\n",
    "    indices = np.arange(num_images)\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        image, label = dataset[idx]\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    \n",
    "    images = torch.stack(images)  # Convertir la lista de imágenes en un tensor\n",
    "    images = images.permute(0, 2, 3, 1).numpy()  \n",
    "\n",
    "\n",
    "    # Mostrar imágenes\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    for idx in range(num_images):\n",
    "        axes[idx].imshow(images[idx])\n",
    "        axes[idx].set_title(f\"Clase: {labels[idx]}\")\n",
    "        axes[idx].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualizar ejemplos del conjunto de entrenamiento\n",
    "visualize_subset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a14b16-69f3-48f2-b4e0-2daafcb640be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShufflePixels:\n",
    "    def __init__(self, seed=42):\n",
    "        \"\"\"\n",
    "        Inicializa la transformación de shuffle de píxeles.\n",
    "        Args:\n",
    "            seed (int): Semilla para generar una permutación fija.\n",
    "        \"\"\"\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(self.seed)  # Fijar la semilla\n",
    "        self.permutation = None\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Aplica la permutación a los píxeles de la imagen.\n",
    "        Args:\n",
    "            img (torch.Tensor): Imagen en formato (C, H, W).\n",
    "        Returns:\n",
    "            torch.Tensor: Imagen con píxeles permutados.\n",
    "        \"\"\"\n",
    "        C, H, W = img.shape  # Extraer dimensiones de la imagen\n",
    "\n",
    "        if self.permutation is None:\n",
    "            # Generar permutación fija\n",
    "            self.permutation = torch.randperm(H * W)  # Permutación fija en 1D\n",
    "\n",
    "        # Aplanar la imagen, permutar y restaurar forma original\n",
    "        img = img.view(C, H * W)  # (C, H * W)\n",
    "        img = img[:, self.permutation]  # Aplicar la permutación\n",
    "        img = img.view(C, H, W)  # Restaurar la forma original\n",
    "        return img\n",
    "\n",
    "# Definir las transformaciones con shuffle de píxeles\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ShufflePixels(seed=42)  # Aplicar shuffle fijo\n",
    "])\n",
    "train_dataset_shuffle = GTS(csv_file='train.csv', root_dir='./data', transform=transform)\n",
    "valid_dataset_shuffle = GTS(csv_file='valid.csv', root_dir='./data', transform=transform)\n",
    "test_dataset_shuffle = GTS(csv_file='test.csv', root_dir='./data', transform=transform)\n",
    "\n",
    "device = 'cpu'\n",
    "batch_size = 16\n",
    "\n",
    "train_loader_shuffle = DataLoader(train_dataset_shuffle, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader_shuffle = DataLoader(valid_dataset_shuffle, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader_shuffle = DataLoader(test_dataset_shuffle, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# Visualizar ejemplos del conjunto de entrenamiento\n",
    "visualize_subset(train_dataset_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14bbab2-1a34-4dd5-a32a-95c1bff7dd16",
   "metadata": {},
   "source": [
    "### Entrena la MLP con:\n",
    "\n",
    "* `train_loader_shuffle`\n",
    "* `valid_loader_shuffle`\n",
    "* `test_loader_shuffle`\n",
    "\n",
    "Guarda el modelo como `mlp_shuffle.pth` y los resultados de `loss` y `accuracy`con la misma extensión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cce0e-903a-4ad1-bb6d-e92d40fa4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MLP\n",
    "\n",
    "# Instancia el modelo\n",
    "model_mlp = \n",
    "\n",
    "# Instancia la función de pérdida y el optimizador\n",
    "loss_fn = \n",
    "optimizer =\n",
    "\n",
    "# Fija 10 épocas\n",
    "num_epochs = 10\n",
    "\n",
    "# Entrena y evalua el modelo. Usa name='mlp_shuffle'\n",
    "train_loss_df, val_loss_df, test_loss, test_accuracy =\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Guarda el modelo. Usa file_name='mlp_shuffle.pth'\n",
    "\n",
    "\n",
    "# Carga los datos y muéstralos\n",
    "train_loss_file = 'train_loss_mlp_shuffle.csv'  \n",
    "valid_loss_file = 'valid_loss_mlp_shuffle.csv'\n",
    "train_accuracy_file = 'train_accuracy_mlp_shuffle.csv'\n",
    "valid_accuracy_file = 'valid_accuracy_mlp_shuffle.csv'\n",
    "\n",
    "plot_loss_accuracy(train_loss_file, valid_loss_file, train_accuracy_file, valid_accuracy_file)\n",
    "\n",
    "# Calcula la matriz de confusión\n",
    "cm = plot_confusion_matrix(model_mlp, test_loader_shuffle, device=device)\n",
    "\n",
    "# Muestra el porcentaje de error por clase\n",
    "plot_error_per_class(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee26846-7eb2-4f51-8b84-fad88fe0bdb5",
   "metadata": {},
   "source": [
    "### Entrena la CNN con:\n",
    "\n",
    "* `train_loader_shuffle`\n",
    "* `valid_loader_shuffle`\n",
    "* `test_loader_shuffle`\n",
    "\n",
    "Guarda el modelo como `cnn_shuffle.pth` y los resultados de `loss`y `accuracy` con la misma extensión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb6107-82d5-4dcb-a08d-c81c7f7d14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia el modelo\n",
    "model_cnn = \n",
    "\n",
    "# Instancia la función de pérdida y el optimizador\n",
    "loss_fn = \n",
    "optimizer = \n",
    "\n",
    "# Fija 10 épocas\n",
    "num_epochs = 10\n",
    "\n",
    "# Entrena y evalua el modelo. Usa name='cnn_shuffle'\n",
    "train_loss_df, val_loss_df, test_loss, test_accuracy = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Guarda el modelo. Usa file_name='cnn_shuffle.pth'\n",
    "\n",
    "\n",
    "# Carga los datos y muéstralos\n",
    "train_loss_file = 'train_loss_cnn_shuffle.csv'  \n",
    "valid_loss_file = 'valid_loss_cnn_shuffle.csv'\n",
    "train_accuracy_file = 'train_accuracy_cnn_shuffle.csv'\n",
    "valid_accuracy_file = 'valid_accuracy_cnn_shuffle.csv'\n",
    "\n",
    "plot_loss_accuracy(train_loss_file, valid_loss_file, train_accuracy_file, valid_accuracy_file)\n",
    "\n",
    "# Calcula la matriz de confusión\n",
    "cm = plot_confusion_matrix(model_cnn, test_loader_shuffle, device=device)\n",
    "\n",
    "# Muestra el porcentaje de error por clase\n",
    "plot_error_per_class(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad29673-8f6f-464b-9756-dea2da61c148",
   "metadata": {},
   "source": [
    "¿Cómo podrías explicar los resultados?\n",
    "\n",
    "############## COMPLETAR ##############\n",
    "\n",
    "############ Fin COMPLETAR ############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea7e2e6-106d-4440-8a27-9f7270ce7b4e",
   "metadata": {},
   "source": [
    "## **[Ejercicio 3]** Entrena una `FCNN` con las siguientes características:\n",
    "\n",
    "1. Cuatro bloques convolucionales (conv2d -> batch norm -> ReLU).\n",
    "2. Un bloque convolucional final con tantas características como clases haya (43)\n",
    "3. Comprueba el número de parámetros entrenables y busca una configuración que no supere los **50.000** parámetros (aproximadamente 4 veces menos que la MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f70e7-e1bd-46b0-bdea-5821ce27030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(FCNN, self).__init__()\n",
    "\n",
    "        # Primer bloque convolucional\n",
    "\n",
    "        # Segundo bloque convolucional\n",
    "        \n",
    "        # Tercer bloque convolucional\n",
    "\n",
    "        # Cuarto bloque convolucional\n",
    "\n",
    "        # Bloque de salida\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Primer bloque convolucional\n",
    "     \n",
    "\n",
    "        # Segundo bloque convolucional\n",
    "    \n",
    "\n",
    "        # Tercer bloque convolucional\n",
    "      \n",
    "\n",
    "        # Cuarto bloque convolucional\n",
    "    \n",
    "\n",
    "        # Capa de salida\n",
    "        x =   # Salida (N, num_classes, 1, 1)\n",
    "        x =   # (N, num_classes)\n",
    "        return x\n",
    "\n",
    "count_trainable_params(FCNN(output_dim=43))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cab23b-5c9a-4998-b4f8-b6b10997c78d",
   "metadata": {},
   "source": [
    "Añade el modelo a tu archivo `models.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3d26a-37b0-48db-91d4-33bd14319515",
   "metadata": {},
   "source": [
    "### Entrena la FCNN con el dataset original (sin permutar)\n",
    "\n",
    "* `train_loader`\n",
    "* `valid_loader`\n",
    "* `test_loader`\n",
    "\n",
    "Guarda el modelo como `fcnn.pth` y los resultados de `loss`y `accuracy` con la misma extensión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9de25d-dbf4-4657-a21f-e1c646601879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia el modelo\n",
    "model_fcnn = \n",
    "\n",
    "# Instancia la función de pérdida y el optimizador\n",
    "loss_fn = \n",
    "optimizer = \n",
    "\n",
    "# Fija 10 épocas\n",
    "num_epochs = 10\n",
    "\n",
    "# Entrena y evalua el modelo. Usa name='fcnn'\n",
    "train_loss_df, val_loss_df, test_loss, test_accuracy =\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Guarda el modelo. Usa file_name='fcnn.pth'\n",
    "\n",
    "\n",
    "# Carga los datos y muéstralos\n",
    "train_loss_file = 'train_loss_fcnn.csv'  \n",
    "valid_loss_file = 'valid_loss_fcnn.csv'\n",
    "train_accuracy_file = 'train_accuracy_fcnn.csv'\n",
    "valid_accuracy_file = 'valid_accuracy_fcnn.csv'\n",
    "\n",
    "plot_loss_accuracy(train_loss_file, valid_loss_file, train_accuracy_file, valid_accuracy_file)\n",
    "\n",
    "# Calcula la matriz de confusión\n",
    "cm = plot_confusion_matrix(model_fcnn, test_loader, device=device)\n",
    "\n",
    "# Muestra el porcentaje de error por clase\n",
    "plot_error_per_class(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29745151-95b0-48d8-ac6b-ca3ca5e9ff25",
   "metadata": {},
   "source": [
    "# **[Ejercicio 4]** Entrena una MLP y una FCNN con el dataset modificado\n",
    "\n",
    "1. Crea un modelo con las características que quieras siempre y cuando no supere los **500.000** parámetros.\n",
    "2. Entrena **30** épocas **SIN** aumentado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd8870-9e43-45e3-aef1-cd506a888a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import random\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "\n",
    "class DeterministicResizeAndPaste:\n",
    "    def __init__(self, canvas_size, scale_range=(0.7, 0.9)):\n",
    "\n",
    "        self.canvas_size = canvas_size if isinstance(canvas_size, tuple) else (canvas_size, canvas_size)\n",
    "        self.scale_range = scale_range\n",
    "\n",
    "    def _get_image_seed(self, img):\n",
    "\n",
    "        img_bytes = img.tobytes()\n",
    "        hash_object = hashlib.md5(img_bytes)\n",
    "        return int(hash_object.hexdigest(), 16)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        seed = self._get_image_seed(img)\n",
    "        rng = random.Random(seed) \n",
    "        scale = rng.uniform(self.scale_range[0], self.scale_range[1])        \n",
    "\n",
    "        orig_w, orig_h = img.size\n",
    "        new_w = int(orig_w * scale)\n",
    "        new_h = int(orig_h * scale)\n",
    "        \n",
    "        img_resized = img.resize((new_w, new_h), Image.BILINEAR)        \n",
    "        canvas = Image.new('RGB', self.canvas_size, (0, 0, 0))\n",
    "        \n",
    "        max_x = max(0, self.canvas_size[1] - new_w)\n",
    "        max_y = max(0, self.canvas_size[0] - new_h)\n",
    "        \n",
    "        pos_x = rng.randint(0, max_x)\n",
    "        pos_y = rng.randint(0, max_y)\n",
    "        \n",
    "        canvas.paste(img_resized, (pos_x, pos_y))        \n",
    "        return canvas\n",
    "\n",
    "# Definir las transformaciones \n",
    "transform = transforms.Compose([\n",
    "    DeterministicResizeAndPaste(canvas_size=32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_dataset_ds2 = GTS(csv_file='train.csv', root_dir='./data', transform=transform)\n",
    "valid_dataset_ds2 = GTS(csv_file='valid.csv', root_dir='./data', transform=transform)\n",
    "test_dataset_ds2 = GTS(csv_file='test.csv', root_dir='./data', transform=transform)\n",
    "\n",
    "device = 'cpu'\n",
    "batch_size = 16\n",
    "\n",
    "train_loader_ds2 = DataLoader(train_dataset_ds2, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader_ds2 = DataLoader(valid_dataset_ds2, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader_ds2 = DataLoader(test_dataset_ds2, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# Visualizar ejemplos del conjunto de entrenamiento\n",
    "visualize_subset(train_dataset_ds2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
