{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b5717af-89e1-4607-918d-dc14b7a103db",
   "metadata": {},
   "source": [
    "#  Práctica Obligatoria - Parte IV - Visualización\n",
    "\n",
    "***<p style=\"text-align:center;\">Aprendizaje Automático II</p>***\n",
    "***<p style=\"text-align:center;\">Visualización</p>***\n",
    "\n",
    "En esta parte, cargarás un modelo CNN entrenado y visualizarás los parámetros y sus activaciones.\n",
    "\n",
    "### Evaluación - 1.5/10 puntos\n",
    "\n",
    "Puntuación de cada parte sobre el total de la práctica:\n",
    "- **[Ejercicio 1]** 0.5 puntos.\n",
    "- **[Ejercicio 2]** 1 punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2b9ef-8964-4517-9203-a37de61cca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from gts_dataset import GTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721cd25-847b-4a45-b419-97c3c666349c",
   "metadata": {},
   "source": [
    "## Importa las funciones que has creado desde `utils.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3cfad-ce31-4709-80ff-69bf7a3517d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import count_trainable_params, train, evaluate, train_and_evaluate, save_full_model, load_full_model, plot_loss_accuracy, plot_confusion_matrix, plot_error_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d0566-d33e-46fa-8c04-a343ac34ae4d",
   "metadata": {},
   "source": [
    "## Carga el dataset GTS  y crea los dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48514552-d2f5-4010-a262-43497d6a1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las transformaciones y conversión a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = GTS(csv_file='train.csv', root_dir='./data', transform=transform)\n",
    "valid_dataset = GTS(csv_file='valid.csv', root_dir='./data', transform=transform)\n",
    "test_dataset = GTS(csv_file='test.csv', root_dir='./data', transform=transform)\n",
    "\n",
    "device = 'cpu'\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f427a-39be-4f81-a60d-8c97898b8165",
   "metadata": {},
   "source": [
    "# Carga el modelo `CNN` entrenado (`cnn.pth`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b630fa-8c00-4a4d-af74-17b7fc511ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CNN\n",
    "\n",
    "model_cnn =  # COMPLETAR\n",
    "\n",
    "model_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c4c52-12df-4be1-870b-d9b409662668",
   "metadata": {},
   "source": [
    "## **[Ejercicio 1]** Visualiza los filtros de la primera capa convolucional y sus activaciones para una imagen dada:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0217c-ce4e-4101-87c0-85b83191f6fe",
   "metadata": {},
   "source": [
    "Para acceder a los filtros, puedes hacer uso de la notación por punto:\n",
    "\n",
    "`model_cnn.conv1.weight.data`\n",
    "\n",
    "Para visualizar las activaciones, deberás ejecutar cada función del bloque convolucional que necesites (conv --> bn --> relu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19dcabe-17b3-424e-8540-8abe7ca8f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_filter_and_activation(model, image, device='cpu'):\n",
    "    \"\"\"\n",
    "    Visualiza la imagen original, un filtro y su activación correspondiente.\n",
    "    \"\"\"\n",
    "    model.eval()  # Asegurar modo evaluación\n",
    "    with torch.no_grad():\n",
    "        ############### COMPLETAR ##############\n",
    "        # Calcular activaciones de la primera capa\n",
    "        activations = \n",
    "\n",
    "    ############### COMPLETAR ##############\n",
    "    # Procesar filtros y activaciones para visualización\n",
    "    filters =      # Filtros de la primera capa\n",
    "\n",
    "    image_np = image.permute(1, 2, 0).cpu().numpy()  # Imagen original en formato HWC para visualizar\n",
    "\n",
    "    # Asegurar valores entre 0 y 1 para la imagen original\n",
    "    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
    "\n",
    "    # Unificar el número de elementos para el slider\n",
    "    num_elements = min(filters.shape[0], activations.shape[0])\n",
    "\n",
    "    def plot_image_filter_activation(index):\n",
    "        \"\"\"\n",
    "        Visualiza la imagen original, un filtro y su activación correspondiente.\n",
    "        \"\"\"\n",
    "        # Configurar la figura\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Mostrar imagen original\n",
    "        axs[0].imshow(image_np)\n",
    "        axs[0].set_title(\"Imagen Original\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        # Mostrar filtro\n",
    "        filter_img = filters[index, :, :, :].permute(1, 2, 0)\n",
    "        filter_img = (filter_img - filter_img.min()) / (filter_img.max() - filter_img.min())\n",
    "        axs[1].imshow(filter_img)\n",
    "        axs[1].set_title(f\"Filtro {index+1}\")\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        # Mostrar activación\n",
    "        activation_img = activations[index, :, :].numpy()\n",
    "        axs[2].imshow(activation_img, cmap='gray')\n",
    "        axs[2].set_title(f\"Activación {index+1}\")\n",
    "        axs[2].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Crear un slider interactivo\n",
    "    interact(plot_image_filter_activation, index=IntSlider(min=0, max=num_elements-1, step=1, description=\"Índice\"))\n",
    "\n",
    "# Ejemplo: Usar la visualización unificada con imagen original\n",
    "example_image, _ = test_dataset[0]  # Seleccionar una imagen del conjunto de prueba\n",
    "plot_filter_and_activation(model_cnn, example_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445685e9-7e7b-4b7b-99fc-a0f800fda5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_image_and_visualize_activations(model, image, device='cpu'):\n",
    "    \"\"\"\n",
    "    Visualiza cómo las activaciones cambian al desplazar la imagen horizontal y verticalmente.\n",
    "    \"\"\"\n",
    "    model.eval()  # Asegurar modo evaluación\n",
    "    with torch.no_grad():\n",
    "        ############### COMPLETAR ##############\n",
    "        # Calcular activaciones de la primera capa\n",
    "        activations = \n",
    "\n",
    "    ############### COMPLETAR ##############\n",
    "    # Procesar filtros y activaciones para visualización\n",
    "    filters =   # Filtros de la primera capa\n",
    "\n",
    "    image_np = image.permute(1, 2, 0).cpu().numpy()  # Imagen original en formato HWC para visualizar\n",
    "\n",
    "    # Asegurar valores entre 0 y 1 para la imagen original\n",
    "    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
    "\n",
    "    def shift_and_plot_activation(horizontal_shift, vertical_shift):\n",
    "        \"\"\"\n",
    "        Traslada la imagen en las direcciones horizontal y vertical, y visualiza el cambio en las activaciones.\n",
    "        \"\"\"\n",
    "        # Desplazar la imagen horizontal y verticalmente\n",
    "        shifted_image = np.roll(image_np, horizontal_shift, axis=1)  # Desplazamiento horizontal\n",
    "        shifted_image = np.roll(shifted_image, vertical_shift, axis=0)  # Desplazamiento vertical\n",
    "\n",
    "        # Configurar la figura\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Mostrar imagen original\n",
    "        axs[0].imshow(image_np)\n",
    "        axs[0].set_title(\"Imagen Original\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        # Mostrar imagen desplazada\n",
    "        axs[1].imshow(shifted_image)\n",
    "        axs[1].set_title(f\"Imagen Desplazada (Horizontal: {horizontal_shift}, Vertical: {vertical_shift})\")\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        # Calcular activaciones para la imagen desplazada\n",
    "        shifted_image_tensor = torch.tensor(shifted_image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "        shifted_activations = F.relu(model.bn1(model.conv1(shifted_image_tensor.to(device)))).cpu().squeeze(0)\n",
    "\n",
    "        # Mostrar activación de la imagen desplazada\n",
    "        activation_img = shifted_activations[0, :, :].detach().numpy()\n",
    "        axs[2].imshow(activation_img, cmap='inferno')\n",
    "        axs[2].set_title(f\"Activación Después Desplazamiento\")\n",
    "        axs[2].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Crear sliders interactivos para el desplazamiento horizontal y vertical\n",
    "    max_shift = 10  # Máximo desplazamiento en píxeles\n",
    "    interact(shift_and_plot_activation,\n",
    "             horizontal_shift=IntSlider(min=-max_shift, max=max_shift, step=1, description=\"Desp. Hor\"),\n",
    "             vertical_shift=IntSlider(min=-max_shift, max=max_shift, step=1, description=\"Desp. Vert\"))\n",
    "    \n",
    "example_image, _ = test_dataset[0]  # Seleccionar una imagen del conjunto de prueba\n",
    "shift_image_and_visualize_activations(model_cnn, example_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0cb93-6c95-47bd-9304-7822d19610c2",
   "metadata": {},
   "source": [
    "## **[Ejercicio 2]** Grad-Cam\n",
    "\n",
    "Haz uno de Grad-Cam para obtener una visualización de qué regiones de la imagen contribuyen más a la predicción del modelo. Grad-Cam realiza, grosso modo, los siguientes pasos:\n",
    "\n",
    "1. **Forward Pass**: Obtener probabilidad de la clase objetivo ($y^c$) así como los mapas de características de la última capa convolucional de tu modelo ($A^k$).\n",
    "2. **Backward Pass**: Calcular gradientes de $y^c$ respecto a los mapas de características ($\\frac{\\partial y^c}{\\partial A^k}$).\n",
    "3. **Cálculo de Pesos ($\\alpha_k$)**: Aplicar *Global Average Pooling* a los gradientes obteniendo un escalar por mapa.\n",
    "4. **Combinación Lineal**: Sumar los mapas de características ($A^k$) ponderados por sus pesos ($\\alpha_k$).\n",
    "5. **ReLU**: Aplicar función de activación ReLU al resultado (eliminar contribuciones negativas).\n",
    "6. **Upsampling**: Redimensionar el mapa de calor de baja resolución al tamaño de la imagen original.\n",
    "7. **Normalización**: Escalar valores entre 0 y 1 para visualización (mapa de calor).\n",
    "\n",
    "Para instalar Grad-Cam, puedes hacer uso de:\n",
    "!pip install grad-cam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
